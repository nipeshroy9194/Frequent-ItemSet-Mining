#this program is not used in this prject
#created for future improvment



#For sorting the data easily we can use java.lang.TreeMap. 

import java.io.*; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.io.LongWritable; 
import org.apache.hadoop.mapreduce.Mapper; 

public class top_10_frequent_Mapper extends Mapper<Object, 
              Text, LongWritable, Text> { 

  // data format => freqitems
  // no_of_freq (tab seperated) 
  @Override
  public void map(Object key, Text value, 
  Context context) throws IOException, 
          InterruptedException 
  { 

    String[] tokens = value.toString().split(" "); 

    String freqitems = tokens[0]; 
    long no_of_freq = Long.parseLong(tokens[1]); 

    no_of_freq = (-1) * no_of_freq; 

    context.write(new LongWritable(no_of_freq), 
              new Text(freqitems)); 
  } 
} 


import java.io.*; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.io.LongWritable; 
import org.apache.hadoop.mapreduce.Reducer; 

public class top_10_frequent_Reducer extends Reducer<LongWritable, 
                  Text, LongWritable, Text> { 

  static int count; 

  @Override
  public void setup(Context context) throws IOException, 
                  InterruptedException 
  { 
    count = 0; 
  } 

  @Override
  public void reduce(LongWritable key, Iterable<Text> values, 
  Context context) throws IOException, InterruptedException 
  { 

    // key         values 
    //-ve of no_of_freq [ freq_items..] 
    long no_of_freq = (-1) * key.get(); 

    String freqitems = null; 

    for (Text val : values) 
    { 
      freqitems = val.toString(); 
    } 

    // we just write 10 records as output 
    if (count < 10) 
    { 
      context.write(new LongWritable(no_of_freq), 
                new Text(freqitems)); 
      count++; 
    } 
  } 
} 
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.io.LongWritable; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.mapreduce.Job; 
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 
import org.apache.hadoop.util.GenericOptionsParser; 

public class Driver { 

  public static void main(String[] args) throws Exception 
  { 
    Configuration conf = new Configuration(); 
    String[] otherArgs = new GenericOptionsParser(conf 
              , args).getRemainingArgs(); 

    // if less than two paths 
    // provided will show error 
    if (otherArgs.length < 2) 
    { 
      System.err.println("Error: please provide two paths"); 
      System.exit(2); 
    } 

    Job job = Job.getInstance(conf, "top_10 program_2"); 
    job.setJarByClass(Driver.class); 

    job.setMapperClass(top_10_frequent_Mapper.class); 
    job.setReducerClass(top_10_frequent_Reducer.class); 

    job.setMapOutputKeyClass(LongWritable.class); 
    job.setMapOutputValueClass(Text.class); 

    job.setOutputKeyClass(LongWritable.class); 
    job.setOutputValueClass(Text.class); 

    FileInputFormat.addInputPath(job, new Path(otherArgs[0])); 
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); 

    System.exit(job.waitForCompletion(true) ? 0 : 1); 
  } 
} 
